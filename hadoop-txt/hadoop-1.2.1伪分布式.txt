其他都不说了，直接开始hadoop的配置，ssh等自己配置


hadoop-1.2.1
路径：/usr/local/hadoop/hadoop-1.2.1

hadoop-env.sh
export JAVA_HOME=/usr/java/jdk1.6.0_45
#目录需要自己创建，并且对hadoop用户赋权限
export HADOOP_PID_DIR=/usr/local/hadoop/pids

core-site.xml
<configuration>
 <property>
   <name>fs.default.name</name>
   <value>hdfs://localhadoop:9000</value>
 </property>
 <property>
   <name>hadoop.tmp.dir</name>
   <value>/home/hadoop/tmp</value>
 </property>
</configuration>

hdfs-site.xml
<configuration>
        <property>  
                <name>dfs.replication</name>  
                <value>1</value>  
        </property>  
        <property>  
                <name>dfs.name.dir</name>  
                <value>/home/hadoop/hdfsname</value>  
        </property>  
        <property>  
                <name>dfs.data.dir</name>  
                <value>/home/hadoop/hdfsdata</value>  
        </property>
		<!-- 为了windows的eclipse连接测试用的 -->
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>
        <property>
                <name>dfs.web.ugi</name>
                <value>wangsx,supergroup</value>
        </property>
</configuration>

mapred-site.xml
<configuration>
        <property>  
                <name>mapred.job.tracker</name>  
                <value>localhost:9001</value>  
        </property>
</configuration>


/etc/profile
#java
export JAVA_HOME=/usr/java/jdk1.6.0_45
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar

#hadoop
export HADOOP_HOME=/usr/local/hadoop/hadoop-1.2.1
export PATH=$PATH:$HADOOP_HOME/bin


格式化操作
hadoop namenode -format

启动hadoop
start-all.sh
